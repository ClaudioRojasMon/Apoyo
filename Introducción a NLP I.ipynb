{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fa7ed7",
   "metadata": {},
   "source": [
    "# Procesamiento Lenguaje Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562bc23-0dc0-4cae-b969-0a4e9f5cd8ca",
   "metadata": {},
   "source": [
    "## ¿Qué es el NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2103428e-448a-46c6-8376-4995ebfc0e7e",
   "metadata": {},
   "source": [
    "```{admonition} ¿Cuando se origina?\n",
    "<div align=\"justify\"> El campo del procesamiento del lenguaje natural comenzó en la década de 1940, después de la Segunda Guerra Mundial. En ese tiempo, se reconoció la importancia de la traducción de un idioma a otro y se esperaba crear una máquina que pudiera hacer esta traducción automáticamente.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817c40a",
   "metadata": {},
   "source": [
    "<div align=\"justify\"> En el <strong>NLP o Natural Language Processing</strong>  se traduce a procesamiento del lenguaje natural y se refiere a la rama de la informática, más concretamente la rama de la inteligencia artificial que se encarga de dar a los ordenadores la capacidad de entender texto de la misma forma que los seres humanos.</div><br>\n",
    "<div align=\"justify\">Combina lingüística computacional el modelo de lenguaje humano basado en reglas con modelos estadísticos de aprendizaje automático y de aprendizaje profundo. Todas estas tecnologías permiten que los ordenadores procesen el lenguaje humano para que parezca que comprenden, generan y responden de manera natural y coherente, como si tuvieran una comprensión real del significado y contexto detrás de las palabras y frases.</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb89da",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"Genesis-and-Evolution-of-NLP.png\" alt=\"NLP\" width=\"700px\">\n",
    "\n",
    "Genesis y Evolución de NLP [Fuente](https://commtelnetworks.com/exploring-the-impact-of-natural-language-processing-on-cni-operations/)\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad916d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Flujo o Pipeline de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00f07d",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es el Pipeline?\n",
    "<div align=\"justify\">Se refiere a una secuencia de etapas o tareas que se aplican secuencialmente al procesar un texto o documento de texto. Cada etapa de la pipeline tiene un propósito específico y procesa el texto de alguna manera antes de pasar los resultados a la siguiente etapa. Las pipelines de NLP son fundamentales para realizar tareas de análisis de texto y extracción de información de manera sistemática y estructurada.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7545e",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"Pipeline.jpg\" alt=\"NLP\" width=\"1000px\">\n",
    "\n",
    "Flujo de Procesamiento de Lenguaje Natural\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ffa37",
   "metadata": {},
   "source": [
    "## Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340e79b-871c-4a9a-875c-974bf71021fe",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es la Vectorización?\n",
    "<div align=\"justify\">Como sabemos, los modelos y algoritmos de aprendizaje automático entienden datos numéricos. La vectorización es un proceso de conversión de datos textuales o categóricos en vectores numéricos. Al convertir los datos en datos numéricos, puede entrenar su modelo con mayor precisión.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a8c45",
   "metadata": {},
   "source": [
    "**¿Por qué necesitamos representar las palabras como vectores numéricos?** \n",
    "\n",
    "- <div align=\"justify\">Cuantificar la semántica de las palabras.</div>\n",
    "- <div align=\"justify\">su significado contextualizado. Hay  palabras que si las aislamos de su contexto, pueden significar dos cosas totalmente diferentes.</div>\n",
    "- <div align=\"justify\">los modelos no pueden procesar texto directamente.</div>\n",
    "\n",
    "\n",
    "<div align=\"justify\">Entonces queremos que haya una representación que pueda captar el significado de una palabra, o sea como lo haría una una persona humana. Lo importante es cuál es la unidad básica que queremos analizar, si son palabras, si son las frases dentro de un párrafo, si son los párrafos dentro de un documento. Podemos representar, por ejemplo, una frase como un único vector numérico.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da9eda2",
   "metadata": {},
   "source": [
    "### Términos de Frecuencia (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "<div align=\"justify\">La cantidad de veces que un determinado token aparece en cada uno de los documentos:</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4143f",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"VF.png\" alt=\"NLP\" width=\"700px\">\n",
    "\n",
    "Ejemplo de Vector de Frecuencia [Fuente](https://old.tacosdedatos.com/texto-vectores)\n",
    "::: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3057e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra 'ejemplo' aparece 1 veces en el texto.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ejemplo de texto\n",
    "texto_ejemplo = \"Este es un ejemplo de texto. En este texto, queremos contar cuántas veces aparece la palabra 'ejemplo'.\"\n",
    "\n",
    "# Tokenizar el texto en palabras\n",
    "palabras = word_tokenize(texto_ejemplo)\n",
    "\n",
    "# Palabra que quieres contar\n",
    "palabra_a_contar = \"ejemplo\"\n",
    "\n",
    "# Contar cuántas veces aparece la palabra en el texto\n",
    "conteo = palabras.count(palabra_a_contar)\n",
    "\n",
    "print(f\"La palabra '{palabra_a_contar}' aparece {conteo} veces en el texto.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd2aa9",
   "metadata": {},
   "source": [
    "### One-hot-encoding (Bag of words)\n",
    "\n",
    "<div align=\"justify\">Si solamente queremos saber si una palabra existe en un documento o no, está la opción de usar la codificación one-hot, que simplemente consiste en colocar verdadero o falso (1 o 0) dependiendo de si la palabra existe o no:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569de272",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"One-hot-encoding.png\" alt=\"One-hot-encoding\" width=\"700px\">\n",
    "\n",
    "Ejemplo de Vector de Frecuencia [Fuente](https://old.tacosdedatos.com/texto-vectores)\n",
    "::: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfb5b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento 1: Este es el primer documento. - Palabra 'documento' presente: True\n",
      "Documento 2: Este es el segundo documento. - Palabra 'documento' presente: True\n",
      "Documento 3: Y aquí tienes el tercer documento. - Palabra 'documento' presente: True\n"
     ]
    }
   ],
   "source": [
    "# Corpus de documentos\n",
    "corpus = [\n",
    "    \"Este es el primer documento.\",\n",
    "    \"Este es el segundo documento.\",\n",
    "    \"Y aquí tienes el tercer documento.\"\n",
    "]\n",
    "\n",
    "# Palabra que deseas buscar\n",
    "palabra_a_buscar = \"documento\"\n",
    "\n",
    "# Función para verificar la existencia de la palabra en cada documento\n",
    "def existe_palabra(documento, palabra):\n",
    "    return palabra in documento.lower()\n",
    "\n",
    "# Crear un vector one-hot para cada documento\n",
    "vectores_one_hot = [existe_palabra(documento, palabra_a_buscar) for documento in corpus]\n",
    "\n",
    "# Mostrar los resultados\n",
    "for i, documento in enumerate(corpus):\n",
    "    print(f\"Documento {i + 1}: {documento} - Palabra '{palabra_a_buscar}' presente: {vectores_one_hot[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8ade9-deb1-475f-bf4b-6ad7a357448c",
   "metadata": {},
   "source": [
    "`````{admonition} Para tener en cuenta\n",
    ":class: tip\n",
    "<div align=\"justify\">Una bolsa de palabras trata a todas las palabras por igual y sólo se preocupa por la frecuencia de palabras únicas en las frases. El TF-IDF da importancia a las palabras de un documento teniendo en cuenta tanto la frecuencia como la unicidad.</div>\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d9fd1-873c-4a7d-b1ca-796c1f96a930",
   "metadata": {},
   "source": [
    "## Librerías para NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27281e",
   "metadata": {},
   "source": [
    "```{table} Librerías de Python para NLP\n",
    ":name: my-table-ref\n",
    "\n",
    "|  Librería |  Descripción| \n",
    "|:-----:|:-----:|\n",
    "| NLTK| Es una biblioteca que admite tareas como clasificación, etiquetado, derivación, análisis y razonamiento semántico.|\n",
    "| Spacy|Permite crear aplicaciones que pueden procesar y comprender grandes volúmenes de texto y admite tokenización para más de 49 idiomas.|\n",
    "| Gensim|Logra implementaciones multinúcleo eficientes de algoritmos como el análisis semántico latente (LSA) y la asignación de Dirichlet latente (LDA).| \n",
    "| Pattern | Es una biblioteca multipropósito que puede manejar NLP, minería de datos, análisis de redes, aprendizaje automático y visualización|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cb89c-2335-4a90-b529-d97d77837ad6",
   "metadata": {},
   "source": [
    "```{dropdown} Documentación de las distintas librerías para NLP\n",
    "[NLTK](https://www.nltk.org) \n",
    "\n",
    "[Spacy](https://spacy.io/api/doc)\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/auto_examples/index.html##documentation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433420ae",
   "metadata": {},
   "source": [
    "### NLTK o Kit de Herramientas de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722fc1b-8127-4614-80d4-f0a0e5b1f6ce",
   "metadata": {},
   "source": [
    "```{admonition} ¿Cuando se origina?\n",
    "<div align=\"justify\"> NLTK fue desarrollado en la Universidad de Pensilvania por Steven Bird y Edward Loper a finales de la década de 1990. Inicialmente se creó como una plataforma para la enseñanza e investigación en lingüística computacional y procesamiento del lenguaje natural (NLP). Con el paso de los años, NLTK ha evolucionado hasta convertirse en una biblioteca indispensable para tareas relacionadas con el lenguaje en los dominios de inteligencia artificial, aprendizaje automático y ciencia de datos.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bff44b",
   "metadata": {},
   "source": [
    "#### Ejercicio con NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si no la has intalado pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ec21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db96a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.downloader import Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d17525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24205e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a446bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import text3 as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3bdfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d28dc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Book of Genesis \n",
      "\n",
      "In the beginning God created the heaven and the earth . And the earth was without form , and void ; and darkness was upon the face of the deep . And the Spirit of God moved upon the face of the waters .\n"
     ]
    }
   ],
   "source": [
    "print(text.name,\"\\n\")\n",
    "extracto = \" \".join(text[:44])\n",
    "print(extracto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
