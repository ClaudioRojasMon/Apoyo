{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fa7ed7",
   "metadata": {},
   "source": [
    "# Procesamiento Lenguaje Natural (NLP) I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562bc23-0dc0-4cae-b969-0a4e9f5cd8ca",
   "metadata": {},
   "source": [
    "## ¿Qué es el NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2103428e-448a-46c6-8376-4995ebfc0e7e",
   "metadata": {},
   "source": [
    "```{admonition} ¿Cuando se origina?\n",
    "<div align=\"justify\">El campo del procesamiento del lenguaje natural comenzó en la década de 1940, después de la Segunda Guerra Mundial. En ese tiempo, se reconoció la importancia de la traducción de un idioma a otro y se esperaba crear una máquina que pudiera hacer esta traducción automáticamente.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817c40a",
   "metadata": {},
   "source": [
    "<div align=\"justify\"> En el <strong>NLP o Natural Language Processing</strong>  se traduce a procesamiento del lenguaje natural y se refiere a la rama de la informática, más concretamente la rama de la inteligencia artificial que se encarga de dar a los ordenadores la capacidad de entender texto de la misma forma que los seres humanos.</div><br>\n",
    "<div align=\"justify\">Combina lingüística computacional el modelo de lenguaje humano basado en reglas con modelos estadísticos de aprendizaje automático y de aprendizaje profundo. Todas estas tecnologías permiten que los ordenadores procesen el lenguaje humano para que parezca que comprenden, generan y responden de manera natural y coherente, como si tuvieran una comprensión real del significado y contexto detrás de las palabras y frases.</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb89da",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"Genesis-and-Evolution-of-NLP.png\" alt=\"NLP\" width=\"700px\">\n",
    "\n",
    "Genesis y Evolución de NLP\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad916d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Flujo o Pipeline de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00f07d",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es el Pipeline?\n",
    "<div align=\"justify\">Se refiere a una secuencia de etapas o tareas que se aplican secuencialmente al procesar un texto o documento de texto. Cada etapa de la pipeline tiene un propósito específico y procesa el texto de alguna manera antes de pasar los resultados a la siguiente etapa. Las pipelines de NLP son fundamentales para realizar tareas de análisis de texto y extracción de información de manera sistemática y estructurada.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ffa37",
   "metadata": {},
   "source": [
    "## Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340e79b-871c-4a9a-875c-974bf71021fe",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es la Vectorización?\n",
    "<div align=\"justify\">Como sabemos, los modelos y algoritmos de aprendizaje automático entienden datos numéricos. La vectorización es un proceso de conversión de datos textuales o categóricos en vectores numéricos. Al convertir los datos en datos numéricos, puede entrenar su modelo con mayor precisión.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a8c45",
   "metadata": {},
   "source": [
    "<div align=\"justify\">¿Por qué necesitamos representar las palabras como vectores numéricos?</div> \n",
    "\n",
    "- <div align=\"justify\">Cuantificar la semántica de las palabras.</div>\n",
    "- <div align=\"justify\">su significado contextualizado. Hay  palabras que si las aislamos de su contexto, pueden significar dos cosas totalmente diferentes.</div>\n",
    "- <div align=\"justify\">los modelos no pueden procesar texto directamente.</div>\n",
    "\n",
    "\n",
    "<div align=\"justify\">Entonces queremos que haya una representación que pueda captar el significado de una palabra, o sea como lo haría una una persona humana. Lo importante es cuál es la unidad básica que queremos analizar, si son palabras, si son las frases dentro de un párrafo, si son los párrafos dentro de un documento. Podemos representar, por ejemplo, una frase como un único vector numérico.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924bdae-b6e8-4a1e-af9b-3b28095f76fb",
   "metadata": {},
   "source": [
    "```{admonition} Para Profundizar \n",
    ":class: tip, dropdown\n",
    "- [De Texto a Vectores](https://www.tacosdedatos.com/ioexception/de-texto-a-vectores-22jd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da9eda2",
   "metadata": {},
   "source": [
    "### Término de Frecuencia (Term Frequency – Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4143f",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué significa el Término de Frecuencia(TF-IDF)?\n",
    "<div align=\"justify\"> <strong>TF-IDF</strong>< puede definirse como el cálculo de cuán relevante es una palabra en una serie o corpus respecto a un texto. La relevancia aumenta proporcionalmente al número de veces que una palabra aparece en el texto, pero se compensa por la frecuencia de la palabra en el corpus (conjunto de datos).</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe6730-f615-46c3-aa9a-ae6451670c85",
   "metadata": {},
   "source": [
    "#### Ejemplo para explicar Término de Frecuencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c6909-ecca-4b5c-b5c8-faf2b3cbd085",
   "metadata": {},
   "source": [
    "Supongamos que tenemos los siguientes tres documentos:\n",
    "\n",
    "- Documento 1: \"Me gusta caminar por el bosque\"\n",
    "- Documento 2: \"Caminar por el bosque es sanador\"\n",
    "- Documento 3: \"Me gusta el bosque\"\n",
    "\n",
    "<div align=\"justify\"><strong>Paso 1:</strong> Calcular la Frecuencia de Término (TF)</div>\n",
    "\n",
    "| Término       | Documento 1                 | Documento 2                 | Documento 3                 |\n",
    "|---------------|-----------------------------|-----------------------------|-----------------------------|\n",
    "| \"Me\"          | 1/5 = 0.2                   | 0                           | 1/4 = 0.25                  |\n",
    "| \"gusta\"       | 1/5 = 0.2                   | 0                           | 0                           |\n",
    "| \"caminar\"     | 1/5 = 0.2                   | 1/5 = 0.2                   | 0                           |\n",
    "| \"por\"         | 1/5 = 0.2                   | 1/5 = 0.2                   | 0                           |\n",
    "| \"el\"          | 0                           | 0                           | 1/4 = 0.25                  |\n",
    "| \"bosque\"      | 1/5 = 0.2                   | 1/5 = 0.2                   | 1/4 = 0.25                  |\n",
    "| \"es\"          | 0                           | 1/5 = 0.2                   | 0                           |\n",
    "| \"sanador\"     | 0                           | 1/5 = 0.2                   | 0                           |\n",
    "\n",
    "<div align=\"justify\"><strong>Paso 2:</strong> Calcular la Frecuencia de Documento Inversa (IDF)</div>\n",
    "\n",
    "- N = 3 (número total de documentos)\n",
    "\n",
    "| Término       | IDF                          |\n",
    "|---------------|------------------------------|\n",
    "| \"Me\"          | \\(\\log(3/2) \\approx 0.176\\)  |\n",
    "| \"gusta\"       | \\(\\log(3/2) \\approx 0.176\\)  |\n",
    "| \"caminar\"     | \\(\\log(3/2) \\approx 0.176\\)  |\n",
    "| \"por\"         | \\(\\log(3/2) \\approx 0.176\\)  |\n",
    "| \"el\"          | \\(\\log(3/1) = \\log(3) \\approx 0.477\\)  |\n",
    "| \"bosque\"      | \\(\\log(3/2) \\approx 0.176\\)  |\n",
    "| \"es\"          | \\(\\log(3/1) = \\log(3) \\approx 0.477\\)  |\n",
    "| \"sanador\"     | \\(\\log(3/1) = \\log(3) \\approx 0.477\\)  |\n",
    "\n",
    "<div align=\"justify\"><strong>Paso 3:</strong> Calcular TF-IDF</div>\n",
    "\n",
    "| Término       | Documento 1                                       | Documento 2                                       | Documento 3                                       |\n",
    "|---------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|\n",
    "| \"Me\"          | \\(0.2 \\cdot 0.176 \\approx 0.035\\)                 | \\(0 \\cdot 0.176 = 0\\)                             | \\(0.25 \\cdot 0.176 \\approx 0.044\\)                 |\n",
    "| \"gusta\"       | \\(0.2 \\cdot 0.176 \\approx 0.035\\)                 | \\(0 \\cdot 0.176 = 0\\)                             | \\(0 \\cdot 0 = 0\\)                                 |\n",
    "| \"caminar\"     | \\(0.2 \\cdot 0.176 \\approx 0.035\\)                 | \\(0.2 \\cdot 0.176 \\approx 0.035\\)                 | \\(0 \\cdot 0 = 0\\)                                 |\n",
    "| \"por\"         | \\(0.2 \\cdot 0.176 \\approx 0.035\\)                 | \\(0.2 \\cdot 0.176 \\approx 0.035\\)                 | \\(0 \\cdot 0 = 0\\)                                 |\n",
    "| \"el\"          | \\(0 \\cdot 0.477 = 0\\)                             | \\(0 \\cdot 0.477 = 0\\)                             | \\(0.25 \\cdot 0.477 \\approx 0.119\\)                |\n",
    "| \"bosque\"      | \\(0.2 \\cdot 0.176 \\approx 0.035\\)                 | \\(0.2 \\cdot 0.176 \\approx 0.035\\)                 | \\(0.25 \\cdot 0.176 \\approx 0.044\\)                 |\n",
    "| \"es\"          | \\(0 \\cdot 0.477 = 0\\)                             | \\(0.2 \\cdot 0.477 \\approx 0.095\\)                 | \\(0 \\cdot 0 = 0\\)                                 |\n",
    "| \"sanador\"     | \\(0 \\cdot 0.477 = 0\\)                             | \\(0.2 \\cdot 0.477 \\approx 0.095\\)                 | \\(0 \\cdot 0 = 0\\)                                 |\n",
    "\n",
    "<div align=\"justify\">En este ejemplo, calculamos primero la frecuencia de cada término en cada documento (TF), luego calculamos la frecuencia de documento inversa (IDF) para cada término y finalmente multiplicamos TF por IDF para obtener los valores de TF-IDF. Esto nos da una medida más equilibrada de la importancia de cada término en cada documento, considerando tanto la frecuencia del término en el documento como su rareza en el corpus total.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd2aa9",
   "metadata": {},
   "source": [
    "### One-hot-encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569de272",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué significa One-hot-encoding?\n",
    "<div align=\"justify\">En la <strong>codificación one-hot</strong><, cada palabra se representa como un vector binario donde solo un bit está configurado en 1 (encendido), y todos los demás están en 0 (apagado). La longitud del vector es igual al tamaño del vocabulario, y cada palabra tiene un índice único en este vector. Este método es simple e intuitivo, pero no captura ninguna relación semántica entre las palabras. Se utiliza comúnmente como una representación básica en modelos de aprendizaje automático.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767bff2-83a1-441a-9833-c5a9ea500b01",
   "metadata": {},
   "source": [
    "#### Ejemplo para explicar One-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034610e-3d9e-4752-bede-5bfc4ced0fdd",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Supongamos que tenemos los siguientes dos documentos:</div>\n",
    "\n",
    "- <div align=\"justify\">Documento 1: \"Me gusta caminar por el bosque\"</div>\n",
    "- <div align=\"justify\">Documento 2: \"Caminar por el bosque es sanador\"</div>\n",
    "\n",
    "<div align=\"justify\"><strong>Paso 1:</strong> creamos un vocabulario a partir de todas las palabras únicas en ambos documentos:</div>\n",
    "\n",
    "\n",
    "- <div align=\"justify\">Vocabulario: [\"Me\", \"gusta\", \"caminar\", \"por\", \"el\", \"bosque\", \"es\", \"sanador\"]</div>\n",
    "\n",
    "<div align=\"justify\"><strong>Paso 2:</strong> representamos cada palabra como un vector binario en el que solo un bit está configurado en 1 y todos los demás en 0. Cada palabra tiene un índice único en este vector.</div>\n",
    "\n",
    "Vocabulario y sus vectores one-hot:\n",
    "\n",
    "- \"Me\": [1, 0, 0, 0, 0, 0, 0, 0]\n",
    "- \"gusta\": [0, 1, 0, 0, 0, 0, 0, 0]\n",
    "- \"caminar\": [0, 0, 1, 0, 0, 0, 0, 0]\n",
    "- \"por\": [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "- \"el\": [0, 0, 0, 0, 1, 0, 0, 0]\n",
    "- \"bosque\": [0, 0, 0, 0, 0, 1, 0, 0]\n",
    "- \"es\": [0, 0, 0, 0, 0, 0, 1, 0]\n",
    "- \"sanador\": [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "<div align=\"justify\"><strong>Paso 2:</strong>: Para representar los documentos usando la codificación one-hot, cada palabra en el documento se sustituye por su vector one-hot correspondiente.</div><br>\n",
    "<div align=\"justify\">Representación one-hot Documento 1: \"Me gusta caminar por el bosque\"</div>\n",
    "\n",
    "  - \"Me\": [1, 0, 0, 0, 0, 0, 0, 0]\n",
    "  - \"gusta\": [0, 1, 0, 0, 0, 0, 0, 0]\n",
    "  - \"caminar\": [0, 0, 1, 0, 0, 0, 0, 0]\n",
    "  - \"por\": [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "  - \"el\": [0, 0, 0, 0, 1, 0, 0, 0]\n",
    "  - \"bosque\": [0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "<div align=\"justify\">Representación one-hot Documento 2: \"Caminar por el bosque es sanador\"</div>\n",
    "\n",
    "  - \"caminar\": [0, 0, 1, 0, 0, 0, 0, 0]\n",
    "  - \"por\": [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "  - \"el\": [0, 0, 0, 0, 1, 0, 0, 0]\n",
    "  - \"bosque\": [0, 0, 0, 0, 0, 1, 0, 0]\n",
    "  - \"es\": [0, 0, 0, 0, 0, 0, 1, 0]\n",
    "  - \"sanador\": [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "<div align=\"justify\">Esta representación muestra cómo cada palabra se convierte en un vector binario único. Sin embargo, como se mencionó, esta técnica no captura ninguna relación semántica entre las palabras.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7141028-cf59-4308-9c73-e8f8582f5199",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33116c-9a59-4471-89b6-0a57352054aa",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué significa Bag of words (BoW)?\n",
    "<div align=\"justify\">En la <strong> Bag Words o Bolsa de Palabras (BoW)</strong>cada documento se representa como un vector donde cada elemento corresponde al conteo de una palabra en el documento. El orden de las palabras se ignora, y solo sus frecuencias importan. Este método preserva más información que la codificación one-hot, pero aún carece de significado semántico y no considera la importancia relativa de las palabras.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e872c4-d91f-49f2-9f8a-26a18c538e62",
   "metadata": {},
   "source": [
    "#### Ejemplo para explicar Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c1d8f-ad76-43e6-b09d-33e51e2b48b0",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Supongamos que tenemos los siguientes dos documentos:</div>\n",
    "\n",
    "- <div align=\"justify\">Documento 1: \"Me gusta caminar por el bosque\"</div>\n",
    "- <div align=\"justify\">Documento 2: \"Caminar por el bosque es sanador\"</div>\n",
    "\n",
    "<div align=\"justify\"><strong>Paso 1:</strong> creamos un vocabulario a partir de todas las palabras únicas en ambos documentos:</div>\n",
    "\n",
    "- <div align=\"justify\">Vocabulario: [\"Me\", \"gusta\", \"caminar\", \"por\", \"el\", \"bosque\", \"es\", \"sanador\"]</div>\n",
    "\n",
    "<div align=\"justify\"><strong>Paso 2:</strong> representamos cada documento como un vector de conteos basado en este vocabulario.</div>\n",
    "\n",
    "<div align=\"justify\">Para el Documento 1:</div>\n",
    "\n",
    "- \"Me\" aparece 1 vez\n",
    "- \"gusta\" aparece 1 vez\n",
    "- \"caminar\" aparece 1 vez\n",
    "- \"por\" aparece 1 vez\n",
    "- \"el\" aparece 1 vez\n",
    "- \"bosque\" aparece 1 vez\n",
    "- \"es\" aparece 0 veces\n",
    "- \"sanador\" aparece 0 veces\n",
    "\n",
    "<div align=\"justify\">Vector para Documento 1: [1, 1, 1, 1, 1, 1, 0, 0]</div><br>\n",
    "\n",
    "<div align=\"justify\">Para el Documento 2:</div>\n",
    "\n",
    "- \"Me\" aparece 0 vez\n",
    "- \"gusta\" aparece 0 vez\n",
    "- \"caminar\" aparece 1 vez\n",
    "- \"por\" aparece 1 vez\n",
    "- \"el\" aparece 1 vez\n",
    "- \"bosque\" aparece 1 vez\n",
    "- \"es\" aparece 1 vez\n",
    "- \"sanador\" aparece 1 vez\n",
    "\n",
    "<div align=\"justify\">Vector para Documento 2: [0, 0, 1, 1, 1, 1, 1, 1]</div><br>\n",
    "\n",
    "<div align=\"justify\"><strong>Paso 3:</strong> la representación BoW para los documentos sería:</div>\n",
    "\n",
    "- Documento 1: [1, 1, 1, 1, 1, 1, 0, 0]\n",
    "- Documento 2: [0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "<div align=\"justify\">En esta representación, se ignora el orden de las palabras y solo se consideran sus frecuencias. Aunque este método preserva más información que la codificación one-hot, no captura las relaciones semánticas entre las palabras ni la importancia relativa de cada una en el contexto. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3057e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra 'ejemplo' aparece 1 veces en el texto.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ejemplo de texto\n",
    "texto_ejemplo = \"Este es un ejemplo de texto. En este texto, queremos contar cuántas veces aparece la palabra 'ejemplo'.\"\n",
    "\n",
    "# Tokenizar el texto en palabras\n",
    "palabras = word_tokenize(texto_ejemplo)\n",
    "\n",
    "# Palabra que quieres contar\n",
    "palabra_a_contar = \"ejemplo\"\n",
    "\n",
    "# Contar cuántas veces aparece la palabra en el texto\n",
    "conteo = palabras.count(palabra_a_contar)\n",
    "\n",
    "print(f\"La palabra '{palabra_a_contar}' aparece {conteo} veces en el texto.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfb5b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento 1: Este es el primer documento. - Palabra 'documento' presente: True\n",
      "Documento 2: Este es el segundo documento. - Palabra 'documento' presente: True\n",
      "Documento 3: Y aquí tienes el tercer documento. - Palabra 'documento' presente: True\n"
     ]
    }
   ],
   "source": [
    "# Corpus de documentos\n",
    "corpus = [\n",
    "    \"Este es el primer documento.\",\n",
    "    \"Este es el segundo documento.\",\n",
    "    \"Y aquí tienes el tercer documento.\"\n",
    "]\n",
    "\n",
    "# Palabra que deseas buscar\n",
    "palabra_a_buscar = \"documento\"\n",
    "\n",
    "# Función para verificar la existencia de la palabra en cada documento\n",
    "def existe_palabra(documento, palabra):\n",
    "    return palabra in documento.lower()\n",
    "\n",
    "# Crear un vector one-hot para cada documento\n",
    "vectores_one_hot = [existe_palabra(documento, palabra_a_buscar) for documento in corpus]\n",
    "\n",
    "# Mostrar los resultados\n",
    "for i, documento in enumerate(corpus):\n",
    "    print(f\"Documento {i + 1}: {documento} - Palabra '{palabra_a_buscar}' presente: {vectores_one_hot[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8ade9-deb1-475f-bf4b-6ad7a357448c",
   "metadata": {},
   "source": [
    "`````{admonition} Para tener en cuenta\n",
    ":class: tip\n",
    "<div align=\"justify\">Una bolsa de palabras trata a todas las palabras por igual y sólo se preocupa por la frecuencia de palabras únicas en las frases. El TF-IDF da importancia a las palabras de un documento teniendo en cuenta tanto la frecuencia como la unicidad.</div>\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d9fd1-873c-4a7d-b1ca-796c1f96a930",
   "metadata": {},
   "source": [
    "## Librerías para NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27281e",
   "metadata": {},
   "source": [
    "```{table} Librerías de Python para NLP\n",
    ":name: my-table-ref\n",
    "\n",
    "|  Librería |  Descripción| \n",
    "|:-----:|:-----:|\n",
    "| NLTK| Es una biblioteca que admite tareas como clasificación, etiquetado, derivación, análisis y razonamiento semántico.|\n",
    "| Spacy|Permite crear aplicaciones que pueden procesar y comprender grandes volúmenes de texto y admite tokenización para más de 49 idiomas.|\n",
    "| Gensim|Logra implementaciones multinúcleo eficientes de algoritmos como el análisis semántico latente (LSA) y la asignación de Dirichlet latente (LDA).| \n",
    "| Pattern | Es una biblioteca multipropósito que puede manejar NLP, minería de datos, análisis de redes, aprendizaje automático y visualización|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cb89c-2335-4a90-b529-d97d77837ad6",
   "metadata": {},
   "source": [
    "```{dropdown} Documentación de las distintas librerías para NLP\n",
    "[NLTK](https://www.nltk.org) \n",
    "\n",
    "[Spacy](https://spacy.io/api/doc)\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/auto_examples/index.html##documentation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433420ae",
   "metadata": {},
   "source": [
    "### NLTK o Kit de Herramientas de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722fc1b-8127-4614-80d4-f0a0e5b1f6ce",
   "metadata": {},
   "source": [
    "```{admonition} ¿Cuando se origina?\n",
    "<div align=\"justify\"> NLTK fue desarrollado en la Universidad de Pensilvania por Steven Bird y Edward Loper a finales de la década de 1990. Inicialmente se creó como una plataforma para la enseñanza e investigación en lingüística computacional y procesamiento del lenguaje natural (NLP). Con el paso de los años, NLTK ha evolucionado hasta convertirse en una biblioteca indispensable para tareas relacionadas con el lenguaje en los dominios de inteligencia artificial, aprendizaje automático y ciencia de datos.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bff44b",
   "metadata": {},
   "source": [
    "#### Ejercicio inicial con NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si no la has intalado pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ec21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db96a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.downloader import Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d17525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24205e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a446bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import text3 as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3bdfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d28dc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Book of Genesis \n",
      "\n",
      "In the beginning God created the heaven and the earth . And the earth was without form , and void ; and darkness was upon the face of the deep . And the Spirit of God moved upon the face of the waters .\n"
     ]
    }
   ],
   "source": [
    "print(text.name,\"\\n\")\n",
    "extracto = \" \".join(text[:44])\n",
    "print(extracto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
