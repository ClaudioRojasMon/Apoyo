{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fa7ed7",
   "metadata": {},
   "source": [
    "# Procesamiento Lenguaje Natural (NLP) II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efec46-a26c-44e4-8661-f3dadafbb881",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a9b0a5-9817-4e05-a890-042ecf4f4f7b",
   "metadata": {},
   "source": [
    "```{admonition} Características de la Data\n",
    "<div align=\"justify\">Una de las primeras cosas necesarias para las tareas de procesamiento del lenguaje natural (NLP) es un corpus. Este corpus deben estar en formato de texto estructurado, como cadenas de caracteres o documentos de texto. Esto puede incluir documentos en formato plano (txt) o formato PDF, documentos HTML, correos electrónicos, publicaciones en redes sociales,entre otros.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12916d2-be9d-4941-9b27-1fc8d8aa7c7a",
   "metadata": {},
   "source": [
    "```{dropdown} Bases de Datos de Corpus para NLP\n",
    "[Blog Authorship Corpus](https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus) \n",
    "\n",
    "[Conjunto de datos de informes de casos legales](https://archive.ics.uci.edu/dataset/239/legal+case+reports)\n",
    "\n",
    "[Conjunto de datos de OpinRank](http://kavita-ganesan.com/entity-ranking-data/#.XQESU9NKgWq)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57d35e-d45e-485b-9d67-d38ac52dd34a",
   "metadata": {},
   "source": [
    "### Ejemplo de descarga de Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53f652-9e33-45c2-b75f-c36867d9fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si no la has intalado pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68806cd6-f7e6-4abe-a521-9b48c807bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704b5466-e09a-4e06-af4b-c7207e399f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.downloader import Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e4ff1c-702e-4a48-960d-6340e435875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5065d7-b7d8-40bd-b413-41f55375b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "484d616c-fb5f-4f74-9a42-f5e457bb30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import text3 as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8f32370-1ed1-483c-a25d-694a468fae7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8108abd6-986e-43e2-84b6-13db5e8a2a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Book of Genesis \n",
      "\n",
      "In the beginning God created the heaven and the earth . And the earth was without form , and void ; and darkness was upon the face of the deep . And the Spirit of God moved upon the face of the waters .\n"
     ]
    }
   ],
   "source": [
    "print(text.name,\"\\n\")\n",
    "extracto = \" \".join(text[:44])\n",
    "print(extracto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c966f-5154-4d97-b625-4da326af192d",
   "metadata": {},
   "source": [
    "### Ejemplo de Corpus Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a638866-324a-4fae-8481-22a64b17113a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25159b-9897-46b6-8130-e3e885bbbb41",
   "metadata": {},
   "source": [
    "### Ejemplo de descarga del Quijote de la Mancha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa7f5c89-51e5-4728-ab96-f9ed38dd727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'Don', 'Quijote', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're', '-', 'use', 'it']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# URL de Project Gutenberg para Don Quijote de la Mancha en español\n",
    "url = 'https://www.gutenberg.org/ebooks/2000.txt.utf-8'\n",
    "\n",
    "# Descargar el archivo de texto\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Guardar el texto en un archivo local\n",
    "with open('don_quijote.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Crear un corpus en NLTK con el texto descargado\n",
    "corpus_root = '.'  # Directorio donde se guarda el archivo de texto\n",
    "wordlists = PlaintextCorpusReader(corpus_root, 'don_quijote.txt', encoding='utf-8')\n",
    "\n",
    "# Ejemplo de uso del corpus\n",
    "print(wordlists.words()[:50])  # Mostrar las primeras 50 palabras del corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b312946",
   "metadata": {},
   "source": [
    "## Pre procesamiento del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd48cbc-016b-4117-9504-e5c8a3c5ffd9",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es el Perocesamiento del Corpus o texto?\n",
    "<div align=\"justify\">El preprocesamiento de datos consiste en una serie de pasos, algunos de los cuales pueden o no aplicarse a una tarea específica, pero generalmente se clasifican en las amplias categorías de tokenización, normalización, eliminación de las stopwords y Lematización o Stemming.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d99fc",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01634321-65ab-4f3b-9a3d-2e9f1c12257e",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es la Tokenización?\n",
    "<div align=\"justify\">Es el proceso de dividir un texto en unidades más pequeñas llamadas \"tokens\". En el contexto del procesamiento del lenguaje natural, los tokens suelen ser palabras individuales o partes más pequeñas de las palabras, como subpalabras o caracteres. La tokenización es una etapa fundamental en el preprocesamiento de texto antes de realizar análisis de texto, minería de datos o tareas de procesamiento de lenguaje natural, ya que permite descomponer el texto en unidades manejables para su posterior procesamiento.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd1d8a-0fee-489b-8298-5cd946040c0d",
   "metadata": {},
   "source": [
    "#### Ejercicio de Tokenización en Inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59511e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0821d",
   "metadata": {},
   "source": [
    "`````{admonition} ¿Qué es algoritmo puntk?\n",
    ":class: tip\n",
    "<div align=\"justify\">Este se utiliza específicamente para la tokenización de oraciones en texto en inglés y otros idiomas. La tokenización de oraciones implica dividir un párrafo o un texto más largo en oraciones individuales. Es un modelo de aprendizaje automático entrenado para identificar los límites de las oraciones en función de patrones lingüísticos y estructurales.</div>\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b35d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea907417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[\"Genesis\" by Biblia]\\nIn the beginning God created the heaven and the earth .', 'And the earth was without form , and void ; and darkness was upon the face of the deep .', 'And the Spirit of God moved upon the face of the waters .'] \n",
      "\n",
      "['[', '``', 'Genesis', \"''\", 'by', 'Biblia', ']', 'In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', 'And', 'the', 'earth', 'was', 'without', 'form', ',', 'and', 'void', ';', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', '.', 'And', 'the', 'Spirit', 'of', 'God', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', '.']\n"
     ]
    }
   ],
   "source": [
    "extracto = \"\"\"\n",
    "[\"Genesis\" by Biblia]\n",
    "In the beginning God created the heaven and the earth . And the earth was without form , and void ; and darkness was upon the face of the deep . And the Spirit of God moved upon the face of the waters .\"\"\"\n",
    "\n",
    "lista_de_frases_0 = sent_tokenize(extracto)\n",
    "print(lista_de_frases_0,\"\\n\")\n",
    "\n",
    "lista_de_palabras_0 = word_tokenize(extracto)\n",
    "print(lista_de_palabras_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "789dc555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', '``', 'Genesis', \"''\", 'by', 'Biblia', ']', 'In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', 'And', 'the', 'earth', 'was', 'without', 'form', ',', 'and', 'void', ';', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', '.', 'And', 'the', 'Spirit', 'of', 'God', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', '.']\n"
     ]
    }
   ],
   "source": [
    "lista_de_palabras_0 = word_tokenize(extracto)\n",
    "print(lista_de_palabras_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "329d5b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n[\"I\\'m nobody!', 'Who are you?\"', \"by Emily Dickinson]\\nI'm nobody!\", 'Who are you?', 'Are you nobody, too?', \"Then there's a pair of us — don't tell!\", \"They'd banish us, you know.\", 'How dreary to be somebody!', 'How public, like a frog\\nTo tell your name the livelong day\\nTo an admiring bog!'] \n",
      "\n",
      "['[', '``', 'I', \"'m\", 'nobody', '!', 'Who', 'are', 'you', '?', \"''\", 'by', 'Emily', 'Dickinson', ']']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "poema_ejemplo = \"\"\"\n",
    "[\"I'm nobody! Who are you?\" by Emily Dickinson]\n",
    "I'm nobody! Who are you?\n",
    "Are you nobody, too?\n",
    "Then there's a pair of us — don't tell!\n",
    "They'd banish us, you know.\n",
    "How dreary to be somebody!\n",
    "How public, like a frog\n",
    "To tell your name the livelong day\n",
    "To an admiring bog!\"\"\"\n",
    "\n",
    "lista_de_frases = sent_tokenize(poema_ejemplo)\n",
    "print(lista_de_frases,\"\\n\")\n",
    "\n",
    "lista_de_palabras = word_tokenize(poema_ejemplo)\n",
    "print(lista_de_palabras[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28aeb065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', '``', 'I', \"'m\", 'nobody', '!', 'Who', 'are', 'you', '?', \"''\", 'by', 'Emily', 'Dickinson', ']', 'I', \"'m\", 'nobody', '!', 'Who', 'are', 'you', '?', 'Are', 'you', 'nobody', ',', 'too', '?', 'Then', 'there', \"'s\", 'a', 'pair', 'of', 'us', '—', 'do', \"n't\", 'tell', '!', 'They', \"'d\", 'banish', 'us', ',', 'you', 'know', '.', 'How', 'dreary', 'to', 'be', 'somebody', '!', 'How', 'public', ',', 'like', 'a', 'frog', 'To', 'tell', 'your', 'name', 'the', 'livelong', 'day', 'To', 'an', 'admiring', 'bog', '!']\n"
     ]
    }
   ],
   "source": [
    "lista_de_palabras = word_tokenize(poema_ejemplo)\n",
    "print(lista_de_palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258cb28-35dc-40dc-8979-c8843829e2b2",
   "metadata": {},
   "source": [
    "#### Ejercicio de Tokenización en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f54e2d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n[\"Poema 20\" by Pablo Neruda]\\nPuedo escribir los versos más tristes esta noche.', 'Escribir, por ejemplo: «La noche está estrellada,\\ny tiritan, azules, los astros, a lo lejos».', 'El viento de la noche gira en el cielo y canta.', 'Puedo escribir los versos más tristes esta noche.', 'Yo la quise, y a veces ella también me quiso.'] \n",
      "\n",
      "['[', '``', 'Poema', '20', \"''\", 'by', 'Pablo', 'Neruda', ']', 'Puedo', 'escribir', 'los', 'versos', 'más', 'tristes']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "poema_ejemplo_2 = \"\"\"\n",
    "[\"Poema 20\" by Pablo Neruda]\n",
    "Puedo escribir los versos más tristes esta noche.\n",
    "Escribir, por ejemplo: «La noche está estrellada,\n",
    "y tiritan, azules, los astros, a lo lejos».\n",
    "El viento de la noche gira en el cielo y canta.\n",
    "Puedo escribir los versos más tristes esta noche.\n",
    "Yo la quise, y a veces ella también me quiso.\"\"\"\n",
    "\n",
    "lista_de_frases_2 = sent_tokenize(poema_ejemplo_2)\n",
    "print(lista_de_frases_2,\"\\n\")\n",
    "\n",
    "lista_de_palabras_2 = word_tokenize(poema_ejemplo_2)\n",
    "print(lista_de_palabras_2[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "911f80a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', '``', 'Poema', '20', \"''\", 'by', 'Pablo', 'Neruda', ']', 'Puedo', 'escribir', 'los', 'versos', 'más', 'tristes', 'esta', 'noche', '.', 'Escribir', ',', 'por', 'ejemplo', ':', '«', 'La', 'noche', 'está', 'estrellada', ',', 'y', 'tiritan', ',', 'azules', ',', 'los', 'astros', ',', 'a', 'lo', 'lejos', '»', '.', 'El', 'viento', 'de', 'la', 'noche', 'gira', 'en', 'el', 'cielo', 'y', 'canta', '.', 'Puedo', 'escribir', 'los', 'versos', 'más', 'tristes', 'esta', 'noche', '.', 'Yo', 'la', 'quise', ',', 'y', 'a', 'veces', 'ella', 'también', 'me', 'quiso', '.']\n"
     ]
    }
   ],
   "source": [
    "lista_de_palabras_2 = word_tokenize(poema_ejemplo_2)\n",
    "print(lista_de_palabras_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65c5c6-0a6c-4ca8-b66f-077b662f6b62",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031478b5-da8b-4690-9cc5-e080cd860999",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es la Normalización?\n",
    "<div align=\"justify\"> Esta coloca todas las palabras en igualdad de condiciones y permite que el procesamiento continúe de manera uniforme. Implica convertir todo el texto al mismo caso (mayúsculas o minúsculas), eliminar puntuaciones, convertir números en sus equivalentes en palabras, entre otras cosas.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f1db5-f5ec-4d65-be58-bb72008c3180",
   "metadata": {},
   "source": [
    "#### Ejemplo 1 de Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8a82da9-39be-4320-9cd9-72f35412e35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      "\n",
      "[\"Poema 20\" by Pablo Neruda]\n",
      "Puedo escribir los versos más tristes esta noche.\n",
      "Escribir, por ejemplo: «La noche está estrellada,\n",
      "y tiritan, azules, los astros, a lo lejos».\n",
      "El viento de la noche gira en el cielo y canta.\n",
      "Puedo escribir los versos más tristes esta noche.\n",
      "Yo la quise, y a veces ella también me quiso.\n",
      "\n",
      "Texto normalizado:\n",
      "\n",
      "poema 20 by pablo neruda\n",
      "puedo escribir los versos más tristes esta noche\n",
      "escribir por ejemplo «la noche está estrellada\n",
      "y tiritan azules los astros a lo lejos»\n",
      "el viento de la noche gira en el cielo y canta\n",
      "puedo escribir los versos más tristes esta noche\n",
      "yo la quise y a veces ella también me quiso\n",
      "\n",
      "Tokens:\n",
      "['poema', '20', 'by', 'pablo', 'neruda', 'puedo', 'escribir', 'los', 'versos', 'más', 'tristes', 'esta', 'noche', 'escribir', 'por', 'ejemplo', '«', 'la', 'noche', 'está', 'estrellada', 'y', 'tiritan', 'azules', 'los', 'astros', 'a', 'lo', 'lejos', '»', 'el', 'viento', 'de', 'la', 'noche', 'gira', 'en', 'el', 'cielo', 'y', 'canta', 'puedo', 'escribir', 'los', 'versos', 'más', 'tristes', 'esta', 'noche', 'yo', 'la', 'quise', 'y', 'a', 'veces', 'ella', 'también', 'me', 'quiso']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"\"\"\n",
    "[\"Poema 20\" by Pablo Neruda]\n",
    "Puedo escribir los versos más tristes esta noche.\n",
    "Escribir, por ejemplo: «La noche está estrellada,\n",
    "y tiritan, azules, los astros, a lo lejos».\n",
    "El viento de la noche gira en el cielo y canta.\n",
    "Puedo escribir los versos más tristes esta noche.\n",
    "Yo la quise, y a veces ella también me quiso.\"\"\"\n",
    "\n",
    "# Convertir a minúsculas\n",
    "texto_miniscula = texto.lower()\n",
    "\n",
    "# Eliminar puntuación\n",
    "texto_sin_puntuacion = texto_miniscula.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Tokenización\n",
    "tokens = word_tokenize(texto_sin_puntuacion)\n",
    "\n",
    "print(\"Texto original:\")\n",
    "print(texto)\n",
    "print(\"\\nTexto normalizado:\")\n",
    "print(texto_sin_puntuacion )\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54f224",
   "metadata": {},
   "source": [
    "#####  Ejercicio 2 de Normalización: lower() vs casefold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3822d1af",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es casefold?\n",
    "<div align=\"justify\"> Es una versión más agresiva de lower() lo que significa que convertirá más caracteres a minúsculas y encontrará más coincidencias al comparar dos cadenas si ambas se convierten utilizando el método casefold(). Es de utilidad especialmente cuando se trabaja con diferentes idiomas.</div>\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a787aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: \t\tmit freundlichen grüßen\n",
      "casefold: \tmit freundlichen grüssen\n"
     ]
    }
   ],
   "source": [
    "texto_ejemplo = \"Mit freundlichen Grüßen\" # \"Atentamente\" en español\n",
    "\n",
    "print(f\"lower: \\t\\t{texto_ejemplo.lower()}\\ncasefold: \\t{texto_ejemplo.casefold()}\")\n",
    "\n",
    "# lower() identifica los caracteres ASCII (256)\n",
    "# casefold() identifica los caracteres UNICODE (143859)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c26bf274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de stopwords:  313\n",
      "Por ejemplo:  ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"spanish\")\n",
    "\n",
    "print(\"Cantidad de stopwords: \", len(stop_words))\n",
    "print(\"Por ejemplo: \", stop_words[:20])\n",
    "print(\"la\" in set(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea2b3cf-cf5d-4e9f-ada5-383efd9b7a8e",
   "metadata": {},
   "source": [
    "### Eliminación de las Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023d453-0906-43ce-8959-58381705f1c6",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué son las Stopwords?\n",
    "<div align=\"justify\">Son palabras de uso común (como \"el\", \"un\", \"una\" o \"en\") que un motor de búsqueda ha sido programado para ignorar, tanto al indexar entradas para la búsqueda como al recuperarlas como resultado de una consulta de búsqueda.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54df2f-c582-4152-893a-42ca76e2cc00",
   "metadata": {},
   "source": [
    "#### Ejemplo de eliminacion Stopwords en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15730c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12eb31d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de stop words:  179\n",
      "Por ejemplo:  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "print(\"Cantidad de stop words: \", len(stop_words))\n",
    "print(\"Por ejemplo: \", stop_words[:10])\n",
    "print(\"then\" in set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75da40c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example remove stopwords English.\n"
     ]
    }
   ],
   "source": [
    "# Obtener la lista de stopwords en inglés\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "# Ejemplo de texto de entrada\n",
    "texto = \"This is an example of how to remove stopwords in English.\"\n",
    "\n",
    "# Tokenizar el texto en palabras\n",
    "palabras = texto.split()\n",
    "\n",
    "# Eliminar las stopwords en inglés\n",
    "palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stopwords_en]\n",
    "\n",
    "# Reconstruir el texto sin las stopwords\n",
    "texto_filtrado = ' '.join(palabras_filtradas)\n",
    "\n",
    "print(texto_filtrado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4cd05-ed77-4a24-ad91-856f816c820a",
   "metadata": {},
   "source": [
    "#### Ejemplo de eliminacion Stopwords en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7070d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ejemplo cómo eliminar stopwords español.\n"
     ]
    }
   ],
   "source": [
    "# Obtener la lista de stopwords en español\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "# Ejemplo de texto de entrada\n",
    "texto = \"Este es un ejemplo de cómo eliminar stopwords en español.\"\n",
    "\n",
    "# Tokenizar el texto en palabras\n",
    "palabras = texto.split()\n",
    "\n",
    "# Eliminar las stopwords en español\n",
    "palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stopwords_es]\n",
    "\n",
    "# Reconstruir el texto sin las stopwords\n",
    "texto_filtrado = ' '.join(palabras_filtradas)\n",
    "\n",
    "print(texto_filtrado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba62a94",
   "metadata": {},
   "source": [
    "#### Ejemplo utilizando tokenización y removiendo stop words en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a47110f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then there's a pair of us — don't tell!\n",
      "['Then', 'there', \"'s\", 'a', 'pair', 'of', 'us', '—', 'do', \"n't\", 'tell', '!'] \n",
      "\n",
      "['Then', 'there', \"'s\", 'pair', 'of', 'us', '—', 'do', \"n't\", 'tell', '!']\n"
     ]
    }
   ],
   "source": [
    "lista_de_frases = sent_tokenize(poema_ejemplo)\n",
    "frase = lista_de_frases[5]\n",
    "print(frase)\n",
    "palabras= word_tokenize(poema_ejemplo)\n",
    "print(palabras, \"\\n\")\n",
    "\n",
    "# utilizando un simple for loop\n",
    "resultado = []\n",
    "for palabra in palabras:\n",
    "   if palabra.casefold() not in stop_words:\n",
    "        resultado.append(palabra)\n",
    "\n",
    "\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ce71e-4f7d-4f3e-898c-f6f96c077e59",
   "metadata": {},
   "source": [
    "#### Ejemplo utilizando tokenización y removiendo stop words en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18e3ea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El viento de la noche gira en el cielo y canta.\n",
      "['El', 'viento', 'de', 'la', 'noche', 'gira', 'en', 'el', 'cielo', 'y', 'canta', '.'] \n",
      "\n",
      "['viento', 'noche', 'gira', 'cielo', 'canta', '.']\n"
     ]
    }
   ],
   "source": [
    "lista_de_frases_2 = sent_tokenize(poema_ejemplo_2)\n",
    "frase = lista_de_frases_2[2]\n",
    "print(frase)\n",
    "palabras= word_tokenize(lista_de_frases_2[2])\n",
    "print(palabras, \"\\n\")\n",
    "\n",
    "# utilizando un simple for loop\n",
    "resultado = []\n",
    "for palabra in palabras:\n",
    "   if palabra.casefold() not in stop_words:\n",
    "        resultado.append(palabra)\n",
    "\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ccb0b-8deb-493c-82a7-e1e993f1f19d",
   "metadata": {},
   "source": [
    "### Stemming y Lemmatizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed649e52-66e0-4671-8d98-e2895319ae46",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b5244-eb48-42bb-a5e6-f03fbb7bde27",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es Stemming?\n",
    "<div align=\"justify\">El stemming es un proceso lingüístico que consiste en la eliminación de sufijos y prefijos de las palabras para reducirlas hasta su raíz o stem. Al eliminar sufijos o prefijos, las palabras de un mismo tema general, como, por ejemplo: corriendo y correra, son cambiadas a “corr”. Generalmente, se emplea para disminuir la complejidad de un texto, facilitando que un software procese y comprenda los patrones de un tema con más claridad.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f4579-6719-472f-8ba3-c66b0745da0c",
   "metadata": {},
   "source": [
    "##### Ejemplo de Stemming en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bf461ae-26bf-41f4-b9b9-52f84dd1abdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras originales:  ['The', 'crew', 'of', 'the', 'USS', 'Discovery', 'discovered', 'many', 'discoveries', '.']\n"
     ]
    }
   ],
   "source": [
    "string_for_stemming = \"\"\"\n",
    "The crew of the USS Discovery discovered many discoveries.\n",
    "Discovering is what explorers do.\"\"\"\n",
    "\n",
    "words = word_tokenize(string_for_stemming)\n",
    "\n",
    "print(\"Palabras originales: \", words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "744d55a2-4207-4060-b919-ec4321c0e51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras stemmizadas:  ['the', 'crew', 'of', 'the', 'uss', 'discoveri', 'discov', 'mani', 'discoveri', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Palabras stemmizadas: \", stemmed_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc176ed0-d8c0-46ff-83dc-3c04c4ad729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n",
      "Palabras stemmizadas:  ['the', 'crew', 'of', 'the', 'uss', 'discoveri', 'discov', 'mani', 'discoveri', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "print(\" \".join(SnowballStemmer.languages))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) # no stem stopwords\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Palabras stemmizadas: \", stemmed_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eba13ac5-bec3-481f-a616-10d21db0bb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "# El stemmer 'english' es mejor que el stemmer 'porter' original (creado en 1979).\n",
    "\n",
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc355745-ab84-4873-a56a-5d0e80f0b225",
   "metadata": {},
   "source": [
    "##### Ejemplo de Stemming en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce95d6ca-3fa4-4334-aa8e-f51c254ec7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra original 1: corriendo\n",
      "Stem 1: corr\n",
      "\n",
      "Palabra original 2: correrá\n",
      "Stem 2: corr\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Configurar el stemmer para español\n",
    "stemmer = SnowballStemmer(language='spanish')\n",
    "\n",
    "# Ejemplo de palabras que deben reducirse al mismo stem\n",
    "palabra1 = \"corriendo\"\n",
    "palabra2 = \"correrá\"\n",
    "\n",
    "# Aplicar stemming\n",
    "stem1 = stemmer.stem(palabra1)\n",
    "stem2 = stemmer.stem(palabra2)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Palabra original 1: {palabra1}\")\n",
    "print(f\"Stem 1: {stem1}\")\n",
    "print(f\"\\nPalabra original 2: {palabra2}\")\n",
    "print(f\"Stem 2: {stem2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55036b-b66f-4731-97e9-0407ed51eea4",
   "metadata": {},
   "source": [
    "#### Lemmatización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f5b88-1e3e-47b9-bd9a-16e6d5cde83a",
   "metadata": {},
   "source": [
    "```{admonition} ¿Qué es la Lemmatización?\n",
    "<div align=\"justify\">La lematización en el procesamiento del lenguaje natural (PNL) es el proceso de reducir las palabras inflectadas (o flexionadas) a su forma base, conocida como lema. El lema es la forma canónica de una palabra, que generalmente se encuentra en un diccionario y representa el significado principal de esa palabra. La lematización utiliza un conocimiento más profundo del idioma y aplica reglas lingüísticas complejas para garantizar que el lema obtenido sea una palabra real y tenga significado. Ejemplo: El lema de \"corriendo\", \"correría\" y \"correrá\" es \"correr\". </div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8e614-2cdf-4277-a711-3413aebdf414",
   "metadata": {},
   "source": [
    "##### Ejemplo de Lemmatización en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d4f35a-0b01-4c14-9d4a-00c8c7471f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lisandro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce24025c-42d3-48fe-8b50-6786e6ff94ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scarv\n",
      "scarf\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(stemmer.stem(\"scarves\"))\n",
    "print(lemmatizer.lemmatize(\"scarves\"))\n",
    "\n",
    "# Al igual que el stemming, la lematización reduce las palabras a su significado principal,\n",
    "# pero le dará una palabra inglesa completa que tiene sentido por sí misma en lugar de\n",
    "# un fragmento de una palabra como 'discoveri'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d866c8-28e6-4872-b4d0-d6841a0960be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst\n",
      "bad\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"worst\"))\n",
    "\n",
    "# lemmatizar como un adjetivo (recordar POS), por defecto es \"n\": noun\n",
    "print(lemmatizer.lemmatize(\"worst\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d04b7e-3fdd-4c85-af57-e0a6523aa5f1",
   "metadata": {},
   "source": [
    "##### Ejemplo de Lematización en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06deb0c5-31e3-440a-8d89-3210e61a1860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/claudiorojas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra original: corriendo => Lema: corriendo\n",
      "Palabra original: correría => Lema: correría\n",
      "Palabra original: correrá => Lema: correrá\n",
      "Palabra original: mejores => Lema: mejores\n",
      "Palabra original: niños => Lema: niños\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Configurar lematizador para español\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Palabras de ejemplo\n",
    "words = [\"corriendo\", \"correría\", \"correrá\", \"mejores\", \"niños\"]\n",
    "\n",
    "# Lematización\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "# Resultados\n",
    "for original, lemmatized in zip(words, lemmatized_words):\n",
    "    print(f\"Palabra original: {original} => Lema: {lemmatized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12067ed3-9b2f-4d15-9fe7-8c22496c85f5",
   "metadata": {},
   "source": [
    "`````{admonition} ¿Cuál es la diferencia entre stemming y lemmatización?\n",
    ":class: tip\n",
    "<div align=\"justify\">El stemming es una técnica de procesamiento de lenguaje natural que se enfoca en reducir las palabras a su raíz básica mediante la eliminación de sufijos y prefijos. Este proceso, conocido como truncamiento, produce formas simplificadas de las palabras, como 'cicl' en lugar de 'ciclista', o 'escri' en vez de 'escribir'. Aunque es eficaz para reducir la complejidad léxica, el stemming a menudo genera raíces que no son palabras reales en el idioma, lo que puede limitar su utilidad en ciertas aplicaciones.</div>\n",
    "\n",
    "<div align=\"justify\">Por otro lado, la lematización es una técnica más sofisticada que utiliza diccionarios y reglas gramaticales para transformar palabras flexionadas a su forma base, o lema. A diferencia del stemming, la lematización tiene en cuenta el contexto lingüístico de una palabra, lo que permite identificar la forma base correcta. Por ejemplo, 'ciclista' se lematizaría a 'ciclista', manteniendo su significado y gramática, mientras que 'escribir', 'escribió' y 'escribiendo' se reducirían correctamente a 'escribir'. Este enfoque más preciso y semánticamente significativo hace que la lematización sea especialmente valiosa para tareas de análisis textual y comprensión del lenguaje natural.</div>\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cea6b7-a360-4e73-8b37-17fbc61ff1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
